{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CMSE 401 - Final Exam\n",
    "This is an open Internet exam.  Feel free to use anything on the Internet with one important exception...\n",
    "\n",
    "- **DO NOT** communicate live with other people during the exam (either verbally or on-line).  The goal here is to find answers to problems as you would in the real world.  \n",
    "\n",
    "You will be given **60 minutes (wishful thinking by the instructor, you will be given the entire exam time if needed)** to complete this Exam.  Use your time wisely. \n",
    "\n",
    "**HINTS:**\n",
    "- Neatness and grammar is important.  We will ignore all notes or code we can not read or understand.\n",
    "- Read the entire exam from beginning to end before starting.  Not all questions are equal in points vs. time so plan your time accordingly.   \n",
    "- Some of the information provided my be a distraction. Do not assume you need to understand everything written to answer the questions. \n",
    "- Spaces for answers are provided. Delete the prompting text such as \"Put your answer to the above question here\" and replace it with your answer. Do not leave the prompting text with your answer.\n",
    "- Do not assume that the answer must be in the same format of the cell provided. Feel free to change the cell formatting (ex. markdown to code) or add additional cells as needed to provide your answer.\n",
    "- When a questions asks for an answer \"**in your own words**\" it is still okay to search the Internet for the answer as a reminder, however, we would like you to do more than cut and paste.  Make the answer your own. \n",
    "- If you get stuck, try not to leave an answer blank. It is better to include some notes or stub functions so we have an idea about your thinking process so we can give you partial credit.   \n",
    "- Always provided links to any references you find helpful. \n",
    "- Feel free to delete the provided check marks (&#9989;) as a way to keep track of which questions you have successfully completed. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exam Summary\n",
    "1. (25 points) [Makefiles](#Q1)\n",
    "2. (25 points) [Distributed Memory](#Q2)\n",
    "2. (25 points) [Shared Memory](#Q3)\n",
    "3. (25 points) [Accelerators](#Q4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"Q1\"></a>\n",
    "# Question 1 - (25 points) Makefiles\n",
    "\n",
    "As we mentioned in class. Makefiles are fairly simple in concept but are an entire programming language so can get very advanced. At its simplest level a makefile consists of rules in the following format:\n",
    "\n",
    "```\n",
    "target : prerequisite_file1 prerequisite_file2 prequiste_file3 ...\n",
    "   #Bash build command(s) to make the target file from the prerequisite files\n",
    "```\n",
    "\n",
    "Makefiles can be thought of as Directed Acyclic Graphs (DAGs).  For example the makefile found in this [file](https://colbrydi.github.io/images//Makefile) can be visualized as the following dependency graph. \n",
    "\n",
    "<img src=\"https://colbrydi.github.io/images/make.png\" width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that files that need to be \"made\" on the same level could be built at the same time. This means we could build them in parallel. For example, in the graph shown above, if we assume that their prerequisite file already exist, ```make2dot.o``` ```y.tab.o``` and ```lex.y.cc``` could can be made at the same time.  While the ```make2dot``` executable can not be made until ```lex.y.o``` is made. \n",
    "\n",
    "The ```make``` program already has a ```-j``` (jobs) option which can be used to leverage the DAG and run in parallel. For example, the following command would run make with four (4) separate tasks running in parallel:\n",
    "\n",
    "    make -j4 \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; <font color=red>**Question 1.a**</font>: Does make use shared memory, shared network, accelerator or a hybrid method for it's parallelization.  How did you come to this answer (if this is a guess, explain your logic)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>Put the answer to the above question here</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; <font color=red>**Question 1.b**</font>: The ```make``` program reports an error if the generated graph produces a dependency cycle.  This means that the arrows in the graph are never allowed to form a loop. This guarantee of an acyclic graph ensures that running ```make``` in parallel will _**not**_ cause a common parallel error when using the ```-j``` option.  What is that common error and why does the acyclic graph prevent it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>Put the answer to the above question here</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The makefile formate is more than the simple rules shown above. For example, the following makefile uses variables, a concept called \"wildcards\" and the [ImageMagic](https://imagemagick.org)  ```convert``` command to take all of the ```png``` files in the current directory and convert them to compressed ```jpg``` files in a folder named images.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "\n",
    "# Creates a list of png images in currect directory\n",
    "INPUT_IMAGES=$(wildcard *.png)\n",
    "\n",
    "# Define the output directory\n",
    "DIR=./images\n",
    "\n",
    "# Define the output image name\n",
    "OUTPUT_IMAGES=$(INPUT_IMAGES:%.png=%.jpg)\n",
    "\n",
    "all: images $(OUTPUT_IMAGES)\n",
    "\n",
    "# Convert a single image\n",
    "%.jpg : %.png\n",
    "\tconvert $< $(DIR)/$@\n",
    "\n",
    "# Create the output directory\n",
    "images : \n",
    "\tmkdir -p images\n",
    "\n",
    "# Delete the output directory\n",
    "clean :\n",
    "\trm -rf images\n",
    "\n",
    "```\n",
    "\n",
    "This makefile creates a very simple DAG where all of the images are on the same level and can be run in parallel. In other words, converting one image to ```jpg``` has nothing to do with converting another image to ```jpg``` because they only have one dependency (i.e. the associated```png``` file) which they do not share. \n",
    "\n",
    "\n",
    "&#9989; <font color=red>**Question 1.c:**</font>:  The above makefile is essentially a pleasantly parallel problem.  If it takes 0.3 seconds to convert a single image, what is a \"best case\" estimation on how long it would take to convert 10,000 images on a 40 core machine using the```make -j40``` command. Make sure you explain your answer by showing your work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>Put the answer to the above question here</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; <font color=red>**Question 1.d**</font>: Now assume that approximately 60% of the time spent converting an image is taken up by File I/O (input and output).  Also assume that due to the serialization of the disk drive, the computer can only read one file at a time (i.e. you can't parallelize the File I/O no matter how many threads you give the problem).  Now, what is a \"best case\" estimation on how long it would take to convert 10,000 images (make sure you explain your answer and/or show your work with units)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>Put the answer to the above question here</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; <font color=red>**Question 1.e**</font>: One common user error when generating a makefile is to forget to include some of the dependencies (prerequisite) required for the build command.   This is a common problem because, ```make``` will not return an error unless a listed dependency doesn't exist or one of the build commands fail. When running in serial, forgetting a dependency in the rule is often not a big deal and the ```make``` command still manages to build the files in the right order.  However, when running in parallel forgetting a dependency may cause what type of problem?  Explain your answer and/or give an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>Put the answer to the above question here</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "<a name=\"Q2\"></a>\n",
    "# Question 2 - (25 points) Distributed Memory Jobs\n",
    "\n",
    "You have been asked to design an agent-based trading model similar to the one we built in [HW6](./HW6_Solution/rumor_mill.c).  Each \"agent\" is a cell on a grid that can only \"see\" it's four neighboring cells (Up, Down, Left Right).  The Principal Investigator (PI or lead researcher) on your project would like to scale this code to fairly large systems and has asked that you split up the world into a grid of of $N$ cells (not slices as we did in the homework solution).  For example, a grid with $N=16$ tasks with four (4) rows and four (4) columns would have the following layout (task ranks are on top (**<font color=blue>blue</font>**), grid coordinates are in parentheses (**black**)):\n",
    "\n",
    "<img src=\"https://computing.llnl.gov/tutorials/mpi/images/Cartesian_topology.gif\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>**Question 2.a:**</font> Given an MPI task's Rank (variable name $rank$), total number of rows (variable name $nrows$), and total number of columns (variable name $ncols$). Write a short piece of C code that would calculate the grid coordinates ($row$ and $col$) of a task (i.e. the calculate the number in the parentheses given the blue number for an arbitrary size grid). For this question you can assume that the total number of tasks ($N$) equals $nrows \\times ncols$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>Put the answer to the above question here</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>**Question 2.b:**</font> Now, given all of the variables we have so far ($rank$, $ncols$, $nrows$, $col$ and $row$). We want to pre-calculate the ranks of a task's neighboring cells. Call these ranks $up$, $down$, $left$ and $right$.  For this example, assume that the boarders do not wrap and that a cell on the edge has \"no neighbor\" which we will indicate with a rank of -1. The following code calculates the $up$ and $down$ variables. Please finish the code to include $left$ and $right$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>Put the answer to the above question in the code cell below</font>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "int up;\n",
    "int down;\n",
    "int left;\n",
    "int right;\n",
    "\n",
    "if (row == 0) {\n",
    "    up = -1\n",
    "else {\n",
    "    up = (row - 1)*ncols+col\n",
    "}\n",
    "\n",
    "if (row == nrows-1) {\n",
    "    up = -1\n",
    "else {\n",
    "    down = (row + 1)*ncols+col\n",
    "}\n",
    "\n",
    "//Put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the Homework solution, each task in the grid maintains a state of the simulation as an array of integers and needs to transmit the state of the array's edges to its neighbors.  The following code (modified from HW6) transmits the edges:\n",
    "\n",
    "```c++\n",
    "    // Send up \n",
    "    if (up == -1) {\n",
    "\t   MPI_Recv(my_world[which][sz_y+1],sz_x+2,MPI_BYTE,down,up_tag,MPI_COMM_WORLD,&status);\n",
    "\t} else {\n",
    "\t   MPI_Send(my_world[which][1],sz_x+2,MPI_BYTE,up,up_tag,MPI_COMM_WORLD);\n",
    "\t   MPI_Recv(my_world[which][sz_y+1],sz_x+2,MPI_BYTE,down,up_tag,MPI_COMM_WORLD,&status);\n",
    "\t}\n",
    "\n",
    "\t// Send down\n",
    "\tif (down == -1) {\n",
    "\t   MPI_Send(my_world[which][sz_y],sz_x+2,MPI_BYTE,down,down_tag,MPI_COMM_WORLD);\n",
    "\t} else {\n",
    "\t   MPI_Send(my_world[which][sz_y],sz_x+2,MPI_BYTE,down,down_tag,MPI_COMM_WORLD);\n",
    "\t   MPI_Recv(my_world[which][0],sz_x+2,MPI_BYTE,up,down_tag,MPI_COMM_WORLD,&status);\n",
    "\t}\n",
    "\n",
    "    // Copy my left vertical vector border #QUESTION 2d\n",
    "    for(int r=1; r < sz_y; r++)\n",
    "       vertical_vec[r] = my_world[which][r][1];\n",
    "\n",
    "    // Send left\n",
    "    if (left != -1)\n",
    "        MPI_Recv(vertical_vec,sz_y,MPI_BYTE,left,left_tag,MPI_COMM_WORLD,&status);\n",
    "\t} else {\n",
    "\t   MPI_Send(vertical_vec,sz_y,MPI_BYTE,left,left_tag,MPI_COMM_WORLD);\n",
    "\t   MPI_Recv(vertical_vec,sz_y,MPI_BYTE,left,left_tag,MPI_COMM_WORLD,&status);\n",
    "\t}\n",
    "    \n",
    "    // Paste left vector to my right boarder #QUESTION 2d\n",
    "    for(int r=1; r < sz_y; r++)\n",
    "       my_world[which][r][sz_x+2] = vertical_vec[r];\n",
    "    \n",
    "    // Copy my right vertical vector boarder  #QUESTION 2d\n",
    "    for(int r=1; r < sz_y; r++)\n",
    "       vertical_vec[r] = my_world[which][r][sz_y+1];\n",
    "\n",
    "    // Send Right\n",
    "    if (left != -1)\n",
    "        MPI_Recv(vertical_vec,sz_y,MPI_BYTE,right,right_tag,MPI_COMM_WORLD,&status);\n",
    "\t} else {\n",
    "\t   MPI_Send(vertical_vec,sz_y,MPI_BYTE,right,right_tag,MPI_COMM_WORLD);\n",
    "\t   MPI_Recv(vertical_vec,sz_y,MPI_BYTE,right,right_tag,MPI_COMM_WORLD,&status);\n",
    "\t}\n",
    "    \n",
    "    // Paste right vector to my left boarder #QUESTION 2d\n",
    "    for(int r=1; r < sz_y; r++)\n",
    "       my_world[which][r][0] = vertical_vec[r];\n",
    "        \n",
    "```\n",
    "\n",
    "&#9989; <font color=red>**Question 2.c**</font>: Assuming the above code works in the context of the entire program.  Explain why the code for transmitting the $left$ and $right$ vertical edges has the extra copy/paste loops (indicated by the ```#QUESTION 2d``` tags in comments above) that we do not need in the up and down."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>Put the answer to the above question here</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; <font color=red>**Question 2.d**</font>: It is probably true that using non-blocking send and receives would be slightly faster.  Explain why you might **NOT** want to use non-blocking messages?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>Put the answer to the above question here</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; <font color=red>**Question 1.e:**</font>: Assume you know that some parts of the task grid has more work to do than others. The fastest time the simulation can run will depend on the slowest component.  For example, the corners (ex: ranks 0, 3, 12 and 15 in the above figure) take only 30 seconds to run per time step; the edges (ex: 1,2,4,7,8,11,13 and 14 in the above figure) take 2 minutes to run per time step and the middle parts (ex: 5, 6, 9 and 10 in the above figure) take 4 minutes to run per time step.  In this case all 16 tasks would take $4 \\text{minutes} \\times \\text{number of time steps}$ to run and most of the tasks would be sitting around doing nothing waiting for the middle tasks.  Describe what you could do to make the entire runtime of the simulation faster? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>Put the answer to the above question here</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"Q3\"></a>\n",
    "\n",
    "# Question 3 - (25 points) Shared Memory Jobs\n",
    "\n",
    "\n",
    "A simple matrix multiply between matrix $A$ (with $i$ rows and $j$ columns) and matrix $B$ (with $j$ rows and $k$ columns) into matrix $C$ (with $i$ rows and $k$ columns) is defined as follows:\n",
    "\n",
    "$$A_{(i\\times j)} \\times B_{(j\\times k)} = C_{(i\\times k)} $$\n",
    "\n",
    "Where each $i,k$ element in $C$ (aka $c_{i,j}$) is calculated using the dot product of the $i^{th}$ row of $A$ by the $j^{th}$ column of $B$:\n",
    "\n",
    "$$c_{i,j} = a_{i,0}b_{0,j} + a_{i,1}b_{1,j} + \\dots + a_{i,k}b_{k,j}$$\n",
    "\n",
    "The following figure is an example depiction of this algorithm:\n",
    "\n",
    "<img src=\"https://lh3.googleusercontent.com/bV-ObD9gwRD4PIcGDEzCP51yGk-f8sx5BUO4JXj9u-KUwYpUJbqRv7A46cwNChKW3S2WD6n3Cg=w500\" width=40%>\n",
    "\n",
    "\n",
    "More information about matrix multiply can be found [here](https://en.wikipedia.org/wiki/Matrix_multiplication).\n",
    "\n",
    "The following C code snip-it can be used to calculate a matrix multiplication between $A$ and $B$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```c\n",
    "//Simple Matrix Multiply\n",
    "for(int i=0;i<sz_i;i++){\n",
    "    for(int k=0;k<sz_k;k++){\n",
    "        C[i][k] = 0;\n",
    "        for(int j=0;k<sz_j;j++){\n",
    "            C[i][k] += A[i][j] * B[j][k];\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; <font color=red>**Question 3.a**</font>: Write an OpenMP pragma for loop to to efficiently parallelize the above matrix?  Where would this command go in the above example?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>Put the answer to the above question here</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; <font color=red>**Question 3.b**</font>:  Which OpenMP scheduling option would best be suited for this problem and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>Put the answer to the above question here</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; <font color=red>**Question 3.c**</font>:  Why would you **NOT** want to use OpenMP if the $A$ and $B$ matrices are really small? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>Put the answer to the above question here</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; <font color=red>**Question 3.d** </font>: Matrix multiply can also be implemented efficiently using GPUs and MPI.  In your own words, explain when each is the best choice for solving a problem and why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color=red>\n",
    "OpenMP - Put your answer here explaining when OpenMP is the best choice for solving matrix multiply and why.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color=red>CUDA - Put your answer here explaining when CUDA is the best choice for solving matrix multiply and why.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color=red>MPI - Put your answer here explaining when MPI is the best choice for solving matrix multiply and why.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; <font color=red>**Question 3.e**</font>:  There are many c/c++ compatible Linear Algebra (LA) program Libraries available that can do matrix multiply.  Find a linear algebra library already installed on the MSU HPCC and provide a link to some documentation for that library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>Put the answer to the above question here</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "<a name=\"Q4\"></a>\n",
    "\n",
    "# Question 4 - (25 points)  CUDA Texture Memory\n",
    "\n",
    "\n",
    "> Like constant memory, texture memory is cached on a GPU's chip, so in some situations it will provide higher effective bandwidth by reducing memory requests to off-chip DRAM. Specifically, texture caches are designed for graphics applications where memory access patterns exhibit a great deal of spatial locality. In a computing application, this roughly implies that a thread is likely to read from an address “near” the address that nearby threads read, as shown in Figure\n",
    "\n",
    "<img src=\"http://1.bp.blogspot.com/-Y-Kga_6J0WU/UQ38DOMpYdI/AAAAAAAAADQ/ejIIjQmKiM8/s640/1.png\">\n",
    "\n",
    "> Note:\n",
    "Arithmetically, the four addresses shown are not consecutive, so they would not be cached together in a typical CPU caching scheme. But since GPU texture caches are designed to accelerate access patterns such as this one, you will see an increase in performance in this case when using texture memory instead of global memory.\n",
    "\n",
    "\n",
    "Consider the following example CUDA code found on stack overflow [REF](https://stackoverflow.com/questions/14334251/cuda-image-average-filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```c++\n",
    "texture<unsigned char, cudaTextureType2D> tex8u;\n",
    "\n",
    "//Box Filter Kernel For Gray scale image with 8bit depth\n",
    "__global__ void box_filter_kernel_8u_c1(unsigned char* output,const int width, const int height, const size_t pitch, const int fWidth, const int fHeight)\n",
    "{\n",
    "    int xIndex = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int yIndex = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "\n",
    "    const int filter_offset_x = fWidth/2;\n",
    "    const int filter_offset_y = fHeight/2;\n",
    "\n",
    "    float output_value = 0.0f;\n",
    "\n",
    "    //Make sure the current thread is inside the image bounds\n",
    "    if(xIndex<width && yIndex<height)\n",
    "    {\n",
    "        //Sum the window pixels\n",
    "        for(int i= -filter_offset_x; i<=filter_offset_x; i++)\n",
    "        {\n",
    "            for(int j=-filter_offset_y; j<=filter_offset_y; j++)\n",
    "            {\n",
    "                //No need to worry about Out-Of-Range access. tex2D automatically handles it.\n",
    "                output_value += tex2D(tex8u,xIndex + i,yIndex + j);\n",
    "            }\n",
    "        }\n",
    "\n",
    "        //Average the output value\n",
    "        output_value /= (fWidth * fHeight);\n",
    "\n",
    "        //Write the averaged value to the output.\n",
    "        //Transform 2D index to 1D index, because image is actually in linear memory\n",
    "        int index = yIndex * pitch + xIndex;\n",
    "\n",
    "        output[index] = static_cast<unsigned char>(output_value);\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "void box_filter_8u_c1(unsigned char* CPUinput, unsigned char* CPUoutput, const int width, const int height, const int widthStep, const int filterWidth, const int filterHeight)\n",
    "{\n",
    "\n",
    "    /*\n",
    "     * 2D memory is allocated as strided linear memory on GPU.\n",
    "     * The terminologies \"Pitch\", \"WidthStep\", and \"Stride\" are exactly the same thing.\n",
    "     * It is the size of a row in bytes.\n",
    "     * It is not necessary that width = widthStep.\n",
    "     * Total bytes occupied by the image = widthStep x height.\n",
    "     */\n",
    "\n",
    "    //Declare GPU pointer\n",
    "    unsigned char *GPU_input, *GPU_output;\n",
    "\n",
    "    //Allocate 2D memory on GPU. Also known as Pitch Linear Memory\n",
    "    size_t gpu_image_pitch = 0;\n",
    "    cudaMallocPitch<unsigned char>(&GPU_input,&gpu_image_pitch,width,height);\n",
    "    cudaMallocPitch<unsigned char>(&GPU_output,&gpu_image_pitch,width,height);\n",
    "\n",
    "    //Copy data from host to device.\n",
    "    cudaMemcpy2D(GPU_input,gpu_image_pitch,CPUinput,widthStep,width,height,cudaMemcpyHostToDevice);\n",
    "\n",
    "    //Bind the image to the texture. Now the kernel will read the input image through the texture cache.\n",
    "    //Use tex2D function to read the image\n",
    "    cudaBindTexture2D(NULL,tex8u,GPU_input,width,height,gpu_image_pitch);\n",
    "\n",
    "    /*\n",
    "     * Set the behavior of tex2D for out-of-range image reads.\n",
    "     * cudaAddressModeBorder = Read Zero\n",
    "     * cudaAddressModeClamp  = Read the nearest border pixel\n",
    "     * We can skip this step. The default mode is Clamp.\n",
    "     */\n",
    "    tex8u.addressMode[0] = tex8u.addressMode[1] = cudaAddressModeBorder;\n",
    "\n",
    "    /*\n",
    "     * Specify a block size. 256 threads per block are sufficient.\n",
    "     * It can be increased, but keep in mind the limitations of the GPU.\n",
    "     * Older GPUs allow maximum 512 threads per block.\n",
    "     * Current GPUs allow maximum 1024 threads per block\n",
    "     */\n",
    "\n",
    "    dim3 block_size(16,16);\n",
    "\n",
    "    /*\n",
    "     * Specify the grid size for the GPU.\n",
    "     * Make it generalized, so that the size of grid changes according to the input image size\n",
    "     */\n",
    "\n",
    "    dim3 grid_size;\n",
    "    grid_size.x = (width + block_size.x - 1)/block_size.x;  /*< Greater than or equal to image width */\n",
    "    grid_size.y = (height + block_size.y - 1)/block_size.y; /*< Greater than or equal to image height */\n",
    "\n",
    "    //Launch the kernel\n",
    "    box_filter_kernel_8u_c1<<<grid_size,block_size>>>(GPU_output,width,height,gpu_image_pitch,filterWidth,filterHeight);\n",
    "\n",
    "    //Copy the results back to CPU\n",
    "    cudaMemcpy2D(CPUoutput,widthStep,GPU_output,gpu_image_pitch,width,height,cudaMemcpyDeviceToHost);\n",
    "\n",
    "    //Release the texture\n",
    "    cudaUnbindTexture(tex8u);\n",
    "\n",
    "    //Free GPU memory\n",
    "    cudaFree(GPU_input);\n",
    "    cudaFree(GPU_output);\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The steps for using texture memory are as follows:\n",
    "\n",
    "1. Declare the texture memory in CUDA.\n",
    "1. Bind the texture memory to your texture reference in CUDA.\n",
    "1. Read the texture memory from your texture reference in CUDA Kernel.\n",
    "1. Unbind the the texture memory from your texture reference in CUDA\n",
    "\n",
    "&#9989; <font color=red>**Question 4.a:**</font> What CUDA Commands are used in the above example to bind and unbind the texture memory?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>Put the answer to the above question here</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; <font color=red>**Question 4.b:**</font> If texture memory is faster, why don't we use it for all CUDA memory access?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>Put the answer to the above question here</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; <font color=red>**Question 4.c:**</font> In your own words, describe why is the ```tex2D``` neeed and what is it doing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>Put the answer to the above question here</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running a device query command in the course BCCD directory results in the following output on dev-intel18:\n",
    "\n",
    "```\n",
    "Device 0:\n",
    "\tname = Tesla K80\n",
    "\tCUDA capability major.minor version = 3.7\n",
    "\tmultiProcessorCount = 13\n",
    "\ttotalGlobalMem = 12799574016 bytes\n",
    "\tsharedMemPerBlock = 49152 bytes\n",
    "\tregsPerBlock = 65536\n",
    "\twarpSize = 32\n",
    "\tmemPitch = 2147483647 bytes\n",
    "\tmaxThreadsPerBlock = 1024\n",
    "\tmaxThreadsDim = 1024 x 1024 x 64\n",
    "\tmaxGridSize = 2147483647 x 65535 x 65535\n",
    "\n",
    "\tmemPitch = 2147483647 bytes\n",
    "\ttextureAlignment = 512 bytes\n",
    "\tclockRate = 0.82 GHz\n",
    "\tdeviceOverlap = Yes\n",
    "\tkernelExecTimeoutEnabled = No\n",
    "\tintegrated = No\n",
    "\tcanMapHostMemory = Yes\n",
    "\tcomputeMode = Default (multiple host threads can use this device simultaneously)\n",
    "\tconcurrentKernels = Yes\n",
    "\tECCEnabled = No\n",
    "\ttccDriver = No\n",
    "```\n",
    "\n",
    "&#9989; <font color=red>**Question 4.d:**</font> How would you change the above code to use the maximum block size on this card?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>Put the answer to the above question here</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; <font color=red>**Question 4.e:**</font> The K80 nodes have 4 K80 cards and each card is equivalent to two k40 cards with the specification shown in Device 0 above.  \n",
    "\n",
    "Assume that your ```main``` program is written to process multiple images at a time.  Also assume the code can is running 8 cores in parallel using openMP and each core is able to talk to a different K40 device (like the one listed above).  Rewrite the following submission script to request 8 cores and 8 GPUs instead of just one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>Modify the following code cell</font>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#!/bin/bash\n",
    "#SBATCH -N 1 #nodes\n",
    "#SBATCH -n 1 #ntasks\n",
    "#SBATCH -c 1 #cpus-per-task\n",
    "#SBATCH -gres gpu:1\n",
    "#SBATCH --time 01:00:00\n",
    "#SBATCH --mem 2gb\n",
    "\n",
    "module load powertools #needed for js shortcut\n",
    "module load cuda\n",
    "\n",
    "time ./multi_GPU_image_average\n",
    "\n",
    "js $SLURM_JOB_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "### Congratulations, you're done with your EXAM\n",
    "\n",
    "Now, you just need to submit this assignment by uploading it to the course <a href=\"https://d2l.msu.edu/\">Desire2Learn</a> web page for today's dropbox.\n",
    "\n",
    "&#9989; <font color=red>**DO THIS:**</font>\n",
    "- Download the Notebook to your desktop with the filename using the format **\"<NETID\\>_Midterm-Exam.ipynb\"**.  Replace <NETID\\> in the filename with your personal MSU NetID (the stuff that comes before the @ symbol in your msu email address).\n",
    "- Upload the newly renamed notebook to the D2L dropbox. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#169; Copyright 2019,  Michigan State University Board of Trustees"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
